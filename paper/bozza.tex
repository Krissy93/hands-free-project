%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
\usepackage{graphicx} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
Hands-Free: a robot augmented reality teleoperation system
}

%\author{\authorblockN{Cristina Nuzzi\authorrefmark{1}, Stefano Ghidini\authorrefmark{2}, Roberto Pagani\authorrefmark{1}, Simone Pasinetti\authorrefmark{1} and Giovanna Sansoni\authorrefmark{1}}
%\authorblockA{\authorrefmark{1} University of Brescia, Department of Mechanical and Industrial Engineering, Via Branze 38, 25123 Brescia, Italy\\
%e-mail: c.nuzzi@unibs.it, r.pagani001@unibs.it, simone.pasinetti@unibs.it, giovanna.sansoni@unibs.it}
%\authorblockA{\authorrefmark{2} STIIMA-CNR, Via Alfonso Corti 12, 20133 Milan, Italy\\
%e-mail: stefano.ghidini@stiima.cnr.it}
%}

\author{Cristina Nuzzi$^{*}$, Stefano Ghidini, Roberto Pagani, Simone Pasinetti and Giovanna Sansoni % <-this % stops a space
\thanks{This work was not supported by any organization.}% <-this % stops a space
\thanks{Cristina Nuzzi, Roberto Pagani, Simone Pasinetti and Giovanna Sansoni are members of the Department of Mechanical and Industrial Engineering at University of Brescia, Via Branze 38, 25123 Brescia, Italy.}
\thanks{Stefano Ghidini is a member of the STIIMA-CNR, Via Alfonso Corti 12, 20133 Milan, Italy. He is also a member of the Department of Mechanical and Industrial Engineering at University of Brescia, Via Branze 38, 25123 Brescia, Italy.}
\thanks{$^{*}$ Corresponding author, e-mail: {\tt\small c.nuzzi@unibs.it}}%
}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

This electronic document is a live template. The various components of your paper [title, text, heads, etc.] are already defined on the style sheet, as illustrated by the portions given in this document.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
Despite advances in robotic perception are increasing autonomous capabilities, the human intelligence is still considered a necessity in unstructured or not predictable environments. Typical scenarios concern the detection of random shape objects, manipulation, or custom robot motion. In such context, human and robots must achieve mutual human-robot interaction (HRI) \cite{Yanco2002}.

HRI can be both physical (pHRI) or not, depending on the assigned task. For example, when the robot is constrained in a dangerous environment or must handle hazardous materials, pHRI is not recommended. In this cases, robot teleoperation may be necessary. A teloperation system concerns with the exploration and exploitation of spaces which do not allow user presence, thus, the operator acts by remotely move the robot \cite{VERTUTJean}. A plenty of human-machine interfaces for teleoperation have been developed considering a mechanical interface, this includes exoskeleton \cite{Rebelo2014} or gloves \cite{Lv2006}. Such systems are particularly helpful to achieve bilateral teleoperation \cite{Hokayem2006}, where they can transmit or reflect back to the user reaction forces from the task being performed. In this case, a high perception with complete haptic feedback \cite{Glover2009} is achieved. Other controllers includes mouse, switchbox, keyboard, touch-screen and joystick, which is usually a better control device than others because the operators can identify better with the task \cite{Boboc2012}. Among the systems where bilateral teleoperation is not required, a teleoperation system is defined by mean of electromyography (EMG) signals of the muscular activity \cite{Vogel2011, Hassan2019}. However, as reminded recently in \cite{Roveda2018a}, EMG could be affected by difficulties in processing EMG signals for amplitude and spectral analysis, reducing their efficiency for many applications. Moreover, all this interfaces still act by contact, hindering the movement of the operator or cause him to act through unnatural movements.

On the other side, if bilater interaction is not required, a vision-based interface is preferable. A vision-based interface does not require physical contact with external devices such as cables, connectors and objects outside of the user working area. This grants a more natural and intuitive interaction, which is reflected on the task performance: as shown in \cite{Kofman2005}, the accuracy of object gripping tasks is improved by mean of a contactless vision-based robot teleoperation method, while in \cite{Livatino2009} a stereo vision system improved the performance of a mobile robot teleoperation application.

Furthermore, if a vision-interface is integrated with virtual and augmented reality techniques, it translates in a greater level of immersion for the user. Such techniques are used to enhance the feedback information; in fact, the operator feels like being physically present in the remote environment, enhancing the immersion level. The notion of immersion is one of the most important reasons for using virtual and augmented reality \cite{Boboc2012}. An augmented reality system for teleoperation based on the Leap Motion (LM) controller is presented in \cite{Peppoloni2015}. For its application domain, the LM controller is considered an accurate sensor \cite{Hedayati2018}, however it is limited to a relatively small measuring distance if compared with other sensors. In this sense, the LM controller introduces spatial constraints that clash with the previously stated concept of the high level user immersion.

For these reasons, we present a novel robot augmented reality teleoperation system that exploits RGB cameras, which provide greater measuring distance if compared with the LM controller. A ROS-based framework has been developed to provide hand tracking and hand-gesture recognition features, exploiting the OpenPose software \cite{simon2017hand, cao2018openpose} based on the Deep Learning framework Caffe \cite{jia2014caffe}.
This, in combination with the ease of availability of an RGB camera, lead the framework to be strongly open-source oriented and highly replicable on all the ROS-based platform. The proposed system includes: neural network for hand$-$gesture recognition (Section \ref{sec:HG_recon}), a rigorous procedure for robot workspace calibration and a mapping policy between the coordinate system of the user and the robot (Section \ref{sec:HG_recon}). 
Different experiments were performed on a \textit{Sawyer (Rethink Robotics)} industrial collaborative robot to evaluate repeatability and accuracy of the proposed system. Section \ref{sec:experiments} reports the results.

\section{Workspace Calibration and Mapping} \label{sec:calib}
\begin{figure*}[h]
\centering
  \includegraphics[width=0.9\textwidth]{figures/horizcalib.png}
  \caption{Scheme of the calibration steps.}
  \label{fig:wscalib}
\end{figure*}
Our set-up is composed of two workspaces: the \textit{user workspace} and the \textit{robot workspace}. Cartesian points in the user workspace that can be reached by the user hand and are correctly viewed by the camera correspond to precise robot end-effector Cartesian points in the robot workspace. To obtain the mapping between the hand positions and the robot end-effector positions, it is necessary to perform a set of calibration procedures described in detail in the following sections.

\subsection{User Workspace Calibration}
In the user workspace an RGB camera is used to recognize the hand skeleton in real-time. Therefore, it is necessary to properly calibrate the camera with respect to the user-defined reference system. This procedure is called \textit{camera calibration}, and can be easily realized following standard procedures, such as the one detailed in \cite{MatlabCameraCalib}.

To correctly map an image point $S = (U, V)$ [px] to its corresponding real world coordinate $P = (X, Y, Z)$ [m] it is necessary to apply the following formula:
\begin{equation}
s 
  \begin{bmatrix}
    U \\
    V \\
    1
  \end{bmatrix}
 =
\mathbf{K} \left( \mathbf{R}
  \begin{bmatrix}
    X \\
    Y \\
    Z \\
  \end{bmatrix}
+ \mathbf{t} \right)
\label{eq1}
\end{equation}

In the equation above, the scalar $s\in\mathbb{R}$ is the scale factor of the image, $\mathbf{K}\in\mathbb{R}^{3\times3}$ is the camera matrix containing the intrinsic parameters of the camera such as focal length and optical center, while matrices $\mathbf{R}\in\mathbb{R}^{3\times3}$ and $\mathbf{t}\in\mathbb{R}^3$ are the rotation and translation matrices of the camera reference system $C$ with respect to the calibration master reference system $H$. When combined, these two represent the rigid transformation matrix $\mathbf{[R|t]}\in\mathbb{R}^{4\times3}$.

To obtain matrix \textbf{K} it is necessary to perform a calibration procedure. A well-performed calibration procedure allows to obtain a satisfactory estimation of the camera parameters. To correctly map image points to the corresponding real world coordinates, the rigid transformation matrix must be estimated with respect to the user-defined reference system of the calibration master. Thus, if reference system $H$ changes or the camera $C$ moves, it is necessary to estimate again the correct rigid transformation matrix. This procedure has been automatized by our software: a calibration script calculates matrix \textbf{K} using a set of calibration images acquired by the user, then it takes a new frame from the actual set-up to estimate the position of reference system $H$.

Images acquired by the camera are processed and the hand skeleton joints coordinates are calculated. By using the abovementioned formula, for each frame it is possible to obtain the real world coordinates of the hand skeleton joints in real time (image frame in Fig. \ref{fig:wscalib}).

%% img del set up di calibrazione matlab
% risultati di calibrazione? Img della mano con skeleton?

\subsection{Robot Workspace Calibration}
The robot workspace refers to the space in which the robot moves (reference system $W$ of Fig. \ref{fig:wscalib}) with respect to the user workspace (reference system $H$ of Fig. \ref{fig:wscalib}). In this case, the user hand real-world position in reference system $H$ is mapped to the new reference system $W$. The mapping between reference system $H$ and reference system $W$ is obtained easily if the two workspaces have the same dimension (matrix \textbf{[R$|$t]} is the identity matrix) or if one workspace is a scaled version of the other one (matrix \textbf{[R$|$t]} is the identity matrix multiplied by the scale factor). %verificare

To correctly move the robot in a cartesian position of reference system $W$, it is necessary to perform a calibration between reference system $W$ and the robot reference system $R$. This procedure has been carried out experimentally by moving the robot (using its manual guidance mode) in different Cartesian positions of reference system $W$. The robot correct positioning on top of each calibration position has been assured by using a 3D-printed centering tool (Fig. \ref{fig:robotcalib}). 

\begin{figure}[h]
	\centering
	\includegraphics[width=1\columnwidth]{figures/robotcalibration.png}
	\caption{Example of the robot calibration procedure. The calibration pose tool (top-right) is placed in correspondence of a calibration marker, then the robot end-effector is carefully placed inside the tool (bottom-left). The calibration pose tool cavity has been purposely made to fit the robot end-effector.}
	\label{fig:robotcalib}
\end{figure}

The tool must be centered manually on each calibration marker and secured in place, then the robot end effector can be moved on it and carefully positioned inside the purposely made circular cavity of the tool. When the positioning is complete, the robot coordinates (both in the Cartesian space and in the Joints space) corresponding to that particular marker (of which the positioning is known with respect to reference system $W$) can be extracted using ROS or the robot proprietary software. 

When a satisfactory number of calibration positions has been acquired, it is possible to estimate the rigid transformation matrix between workspaces $W$ and $R$. Considering the calibration example in Fig. \ref{master_robot}, the position of point $\mathbf{P}_1\in\mathbb{R}^2$ is calculated with respect to both frame $W$ ($P_{1,W}$) and frame $R$ ($P_{1,R}$).

\begin{figure}[h]
	\centering
	\includegraphics[width=1\columnwidth]{figures/calibration.png}
	\caption{Master used to calibrate the second user-defined reference system $W$ with the robot reference frame $R$. The Figure illustrate the position of  point $\mathbf{P_1}$ in both reference frames. To properly calibrate the system, the position of each point is required, both for frame $W$ and $R$. In the context, 13 calibration points have been used.}
	\label{fig:master_robot}
\end{figure}

The distance between the two reference frames is $\mathbf{T}_R\in\mathbb{R}^2$, hence:
\begin{equation}
\mathbf{P}_{0,R} = \mathbf{P}_{0,W} + \mathbf{T}_R
\end{equation}
Using homogeneous coordinates it is possible to rewrite the previous equation as matrix products:
\begin{equation}\label{eq:matrix_M}
\mathbf{P}_{0,R} = \mathbf{M}^W_{R}\mathbf{P}_{0,W}
\end{equation}
Where $\mathbf{M}^W_{R}\in\mathbb{R}^{3\times3}$ is the rigid transformation matrix between the two reference systems. By evidencing equations from Equation \ref{eq:matrix_M}, we obtain:
\begin{equation}\label{eq:linear_sys1}
\begin{gathered}
x_{P_{0,W}}=x_{P_{0,R}}\cos\theta+y_{P_{0,R}}\sin\theta+x_{T_R}\\
y_{P_{0,W}}=-x_{P_{0,R}}\sin\theta+y_{P_{0,R}}\cos\theta+y_{T_R}
\end{gathered}
\end{equation}
The aim of the calibration procedure is to calculate $\mathbf{M}^W_{R}$ in order to find the correct position and orientation of frame $W$ with respect to frame $R$. However, considering only one calibration position point $\mathbf{P1}$, the system in Equation \ref{eq:linear_sys1} results underdetermined, hence, a minimum of $n>2$ calibration points is required to solve the system. To minimize the calibration error, $n=13$ points have been considered. Thus, the system in Equation \ref{eq:linear_sys1} becomes an overdetermined system $\mathbf{A}\mathbf{x}=\mathbf{b}$ that is solved using the least square method:

\begin{equation}\label{eq:linear_sys2}
\resizebox{\columnwidth}{!}{$%
	\begin{gathered}
	\mathbf{A} =   \begin{bmatrix}
	x_{P_{0,R}} & y_{P_{0,R}} & 1 & 0\\
	-y_{P_{0,R}} & x_{P_{0,R}} & 0 & 1 \\
	\vdots & \vdots & \vdots & \vdots\\
	x_{P_{n-1,R}} & y_{P_{n-1,R}} & 1 & 0\\
	-y_{P_{n-1,R}} & x_{P_{n-1,R}} & 0 & 1 
	\end{bmatrix}
	\mathbf{x} =   \begin{bmatrix}
	\cos\theta\\
	\sin\theta\\
	x_{T_R}\\
	y_{T_R}
	\end{bmatrix}
	\mathbf{b} =   \begin{bmatrix}
	x_{P_{0,W}}\\
	y_{P_{0,W}}\\
	\vdots\\
	x_{P_{n-1,W}}\\
	y_{P_{n-1,W}}\\
	\end{bmatrix}
	\end{gathered}
	$%
}%
\end{equation}

Using the terms of $\mathbf{x}$ calculated from Equation \ref{eq:linear_sys2}, we obtain the rigid transformation matrix $\mathbf{M}^W_{R}$.

\section{Hand-Gesture Recognition}\label{sec:HG_recon}
The proposed teleoperation method is based on the recognition of the user hands skeleton.

Each frame acquired by the RGB camera (in our set-up, a Kinect v2 camera) is processed by the software, which leverages the OpenPose hand skeleton recognition network to predict the hand skeleton, following the details of \cite{simon2017hand}.
The gesture recognition procedure is based on the position of the reference keypoint (red keypoint $0$ in Fig. \ref{fig:keypoints}) and on the position of the four knuckles keypoints (blue keypoints $5, 9, 13, 17$ in Fig. \ref{fig:keypoints}). According to the knuckles and the thumb $(x, y)$ keypoints positions (from keypoint $1$ to $4$), the hand orientation can be hypotized as upright, left oriented, right oriented, or upside down. This allows the software to recognize the gestures regardless of the hand orientation.

To recognize if a certain finger is opened or closed, we consider the Euclidean distance between the reference keypoint $0$ and the last keypoint of each finger (pink keypoints $8, 12, 16, 20$ in Fig. \ref{fig:keypoints}). If the last keypoint of a finger is not recognized by the network, we consider the corresponding Euclidean distance equal to $0$. 
Using this logic, we defined two gestures used to carry out basic teleoperation tasks: the \textbf{open hand} gesture, where all the fingers are detected as opened, and the \textbf{index} gesture, where the index finger is detected as open and with a corresponding Euclidean distance much greater than the Euclidean distances of the other fingers. This requirement has been proved useful to reduce the recognition error of the index gesture due to a wrong prediction of the fingers keypoints.

\begin{figure} [h!]
  \centering
  \includegraphics[width=0.7\columnwidth]{figures/hand.png}
  \caption{Example of an open hand gesture correctly recognized skeleton. The red keypoint is the reference keypoint, the blue keypoints are the knuckles keypoints and the pink keypoints are the fingertips keypoints.}
  \label{fig:keypoints}
\end{figure}

Considering the calibration procedure detailed in Section \ref{sec:calib}, a certain position $P_H$ of user workspace H corresponds to a certain robot end-effector position $P_W$ in workspace W. Hence, to move the robot end-effector in position $P_W$  using the software, users must: %img
\begin{enumerate}
\item place their hand in position $P_H$ (corresponding to position $P_W$), using the real-time visualization of the software as guidance (Fig. xx a);
\item perform the open hand gesture to allow the coordinate extraction (Fig. xx b);
\item perform the index gesture, carefully pointing the index finger to position $P_H$ (Fig. xx c).
\end{enumerate}

It is worth noting that, since the hand skeleton is obtained by a neural network which estimates the joints coordinates frame per frame, their position in consectuive frames may vary. Therefore, our software extracts $N$ different $P_H$ coordinates from $N$ consecutive index gestures recognized in consecutive frames. The average coordinates are extracted to reduce positioning errors introduced by the hand skeleton recognition network. The higher the value of $N$, the higher the error reduction, at the cost of a higher delay before the final $P_H$ coordinates are extracted. In our set-up, we set $N = 7$.
After a position $P_H$ is obtained, the corresponding robot position $P_R$ is calculated and the robot is moved there using ROS.
To perform a new robot movement, the procedure in Fig. xx must be repeated from the start. %%

\section{EXPERIMENTAL EVALUATION}\label{sec:experiments}
A reliable teleoperation system is obtained if the robot correctly moves to the desired position with a low positioning error. In the case of the proposed set-up, the positioning error seen when teleoperating our robot is obtained as a sum of different errors, as shown in Fig. \ref{fig:wscalib}. %

First, when the user points the index finger to a Cartesian point in workspace $H$, OpenPose neural network estimates the index position in the image (keypoint $8$ in Fig. \ref{fig:keypoints}). According to the filtering adopted and explained in Section \ref{sec:HG_recon}, the correspoding point is:
\begin{equation}
P_O = \frac{\sum_{n=1}^{N}P_{On}}{N}
\end{equation}

The extracted index position $P_O$ corresponds to an image point $P_C$ plus an estimation error $E_{sk}$, obtained as the pixel distance between the real index position ($P_C$) and the estimaed index keypoint position ($P_O$). Hence: 
\begin{equation}
P_C = P_O + E_{sk} = \frac{\sum_{n=1}^{N}P_{On}}{N} + E_{sk}
\end{equation}

Thanks to the camera calibration procedure, a point in the acquired image frame $P_C$ [px] corresponds to a Cartesian point in workspace $H$, $P_C'$ [m]. $P_C'$ corresponds to the real position $P_H$ of the original $P_C$ plus an estimation error $E_H$ which depends on the accuracy of the calibration:
\begin{equation}
P_H = P_C' + E_H = (P_O + E_{sk})' + E_H
\end{equation}
Since the image point we consider is $P_O$, we obtain: 
\begin{equation}
P_H = (\frac{\sum_{n=1}^{N}P_{On}}{N} + E_{sk})' + E_H
\end{equation}
where the apex represents the conversion from pixels to meters (Section \ref{sec:calib}).

Since the workspace where we want the robot to move is workspace $W$, we must obtain the position of $P_W$, which corresponds to $P_H$ according to the specific mapping between the two workspaces. We obtain:
\begin{equation}
P_W = P_H + E_{map} = (\frac{\sum_{n=1}^{N}P_{On}}{N} + E_{sk}) + E_H + E_{map}
\end{equation}
where $E_{map}$ is the error caused by this mapping.

Finally, to correctly move the robot end-effector to $P_W$, we must obtain the corresponding $P_R$ in the robot reference system $R$. This correspondence is obtained from the robot calibration procedure detailed in Section \ref{sec:calib}, therefore: 
\begin{equation}
P_R = P_W + E_W
\end{equation}
\begin{equation}
P_R = (\frac{\sum_{n=1}^{N}P_{On}}{N} + E_{sk})' + E_H + E_{map} + E_W
\end{equation}

Considering the last formula, and that in our set-up $E_{map}$ can be assumed equal to zero (because we kept workspace $H$ and workspace $W$ dimensions except for the scaling factor), we designed two experiments to assess if the positioning error obtained depends (i) on the estimation of the hand skeleton ($E_{sk}$), (ii) on the camera calibration ($E_H$) or (iii) on the robot workspace calibration ($E_W$).

\subsection{Evaluation of the skeleton estimation error}
The positioning error due to the estimation of the hand skeleton joints made by OpenPose neural network has been evaluated considering the theoretical position of the index in the image $T$ and the index joint position in the image $A$ calculated by the software (Fig. xx).
%% risultati e commento

\subsection{Evaluation of the robot positioning error}
In our set-up, reference system $H$ is placed horizontally with the camera mounted still at a $1 m$ distance (Fig. xx). Reference system $W$, however, has been placed vertically on a glass pane (Fig. xx). %%

To reliably assess the positioning of the robot end-effector, a 3D printed carrier holding a bright red laser has been mounted on the end effector (\textit{Lasiris laser 635nm, 10mW}) (Fig. \ref{fig:laser}, bottom-right corner).

When the robot is moved to a certain theoretical position $T$, the laser will point to its actual positioning $A$. To correctly visualize and measure the robot workspace and the laser positioning, an RGB camera (\textit{IDS Imaging UI-1460C}) has been mounted behind the glass pane.
A measuring software has been developed using LabVIEW to measure the distance ($E$) between the theoretical position $T$ (calculated as the barycenter of the black square of the experimental master, represented as the green dot in Fig. \ref{fig:laser}, top-left corner) and the actual positioning $A$ (red dot in Fig. \ref{fig:laser}, top-left corner).

\begin{figure} [h!]
  \centering
  \includegraphics[width=0.7\columnwidth]{figures/lasermount.png}
  \caption{Example of a robot positioning evaluation. The image shows a close-up of the 3D printed laser carrier used (bottom-right) and the corresponding view of the measuring software developed (top-left).}
  \label{fig:laser}
\end{figure}

We moved the robot in $5$ theoretical positions, corresponding to markers A4, A5, O0, B4 and B5, for a total of $3$ times per each theoretical position. To do that, we used the Cartesian positions corresponding to the circular markers barycenters (theoretical positions) to move the robot using ROS. Hence, this procedure avoids considering the hand skeleton estimation errors.
The resulting distances in millimeters are reported in Table \ref{tab:laser}.

\begin{table}[h]
\begin{center}
\caption{Robot positioning errors}
\label{tab:laser}
\begin{tabular}{|c||c||c||c||c|}
\hline
\textbf{A4} & \textbf{A5} & \textbf{O0} & \textbf{B4} & \textbf{B5} \\
\hline
6.84 & 6.89 & 7.75 & 6.27 & 8.86 \\
6.70 & 5.64 & 7.88 & 6.48 & 5.64\\
6.99 & 5.29 & 7.87 & 6.77 & 5.29\\
\hline
\end{tabular}
\end{center}
\end{table}

We computed the resulting mean distance and the standard deviation of each marker position, reported in Table \ref{tab:mean}. The average mean distance and the average standard deviation achieved by the robot are $6.74$ mm and $0.53$ mm respectively.

\begin{table}[h]
\begin{center}
\caption{Mean and standard deviation}
\label{tab:mean}
\begin{tabular}{|c||c||c||c||c|}
\hline
\textbf{A4} & \textbf{A5} & \textbf{O0} & \textbf{B4} & \textbf{B5} \\
\hline
6.84 & 5.94 & 7.84 & 6.51 & 6.60 \\
0.12 & 0.69 & 0.06 & 0.21 & 1.61 \\
\hline
\end{tabular}
\end{center}
\end{table}

\section{CONCLUSIONS}

Conclusioni sul progetto/esperimenti ottenuti.
Problematiche incontrate e come sono state risolte.
Future developments.

\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.
%Hands-free
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\bibliographystyle{ieeetr}
%\bibliography{/home/abel/Documenti/Mendeley_References/LuGre}


\bibliographystyle{ieeetr}
\bibliography{bibliography}


\end{document}
