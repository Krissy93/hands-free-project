%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

%\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

\documentclass[a4paper, 10 pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
\usepackage{graphicx} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{float}
\usepackage{multicol}
\usepackage{adjustbox}
\usepackage{stfloats}

\title{\LARGE \bf
Hands-Free: a robot augmented reality teleoperation system
}

%\author{\authorblockN{Cristina Nuzzi\authorrefmark{1}, Stefano Ghidini\authorrefmark{2}, Roberto Pagani\authorrefmark{1}, Simone Pasinetti\authorrefmark{1} and Giovanna Sansoni\authorrefmark{1}}
%\authorblockA{\authorrefmark{1} University of Brescia, Department of Mechanical and Industrial Engineering, Via Branze 38, 25123 Brescia, Italy\\
%e-mail: c.nuzzi@unibs.it, r.pagani001@unibs.it, simone.pasinetti@unibs.it, giovanna.sansoni@unibs.it}
%\authorblockA{\authorrefmark{2} STIIMA-CNR, Via Alfonso Corti 12, 20133 Milan, Italy\\
%e-mail: stefano.ghidini@stiima.cnr.it}
%}

\author{Cristina Nuzzi$^{*}$, Stefano Ghidini, Roberto Pagani, Simone Pasinetti and Giovanna Sansoni % <-this % stops a space
\thanks{This work was not supported by any organization.}% <-this % stops a space
\thanks{Cristina Nuzzi, Roberto Pagani, Simone Pasinetti and Giovanna Sansoni are members of the Department of Mechanical and Industrial Engineering at University of Brescia, Via Branze 38, 25123 Brescia, Italy.}
\thanks{Stefano Ghidini is a member of the STIIMA-CNR, Via Alfonso Corti 12, 20133 Milan, Italy. He is also a member of the Department of Mechanical and Industrial Engineering at University of Brescia, Via Branze 38, 25123 Brescia, Italy.}
\thanks{$^{*}$ Corresponding author, e-mail: {\tt\small c.nuzzi@unibs.it}}%
}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

This electronic document is a live template. The various components of your paper [title, text, heads, etc.] are already defined on the style sheet, as illustrated by the portions given in this document.This electronic document is a live template. The various components of your paper [title, text, heads, etc.] are already defined on the style sheet, as illustrated by the portions given in this document.This electronic document is a live template. The various components of your paper [title, text, heads, etc.] are already defined on the style sheet, as illustrated by the portions given in this document.


\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
Despite advances in robotic perception are increasing autonomous capabilities, the human intelligence is still considered a necessity in unstructured or not predictable environments. Typical scenarios concern the detection of random shape objects, manipulation, or custom robot motion. In such context, human and robots must achieve mutual Human-Robot Interaction (HRI) \cite{Yanco2002}. 

HRI can be physical (pHRI) or not, depending on the assigned task. For example, when the robot is constrained in a dangerous environment or must handle hazardous materials, pHRI is not recommended. In these cases, robot teleoperation may be necessary. A teloperation system concerns with the exploration and exploitation of spaces where the user presence is not allowed. Therefore, the operator needs to move the robot remotely \cite{VERTUTJean}. A plenty of human-machine interfaces for teleoperation have been developed considering a mechanical interface, including exoskeletons \cite{Rebelo2014} or gloves \cite{Lv2006}. Such systems are particularly helpful to achieve bilateral teleoperation \cite{Hokayem2006}, where they can transmit or reflect back to the user reaction forces from the task being performed. In this case, a high perception with complete haptic feedback \cite{Glover2009} is achieved. Other controllers may include mouses, switchboxes, keyboards, touch-screens and joysticks. Joysticks in particular are usually a better type of control device than others, because the operators better identify with the task \cite{Boboc2012}. Among the systems where bilateral teleoperation is not required, a teleoperation system is defined by means of electromyography (EMG) signals of the muscular activity \cite{Vogel2011, Hassan2019}. However, as recently highlighted in \cite{Roveda2018a}, the electromyography may be affected by difficulties in processing the EMG signals for amplitude and spectral analysis, reducing the efficiency of the method for many applications. Moreover, all these interfaces still act by contact, hindering the movement of the operator or cause him to act through unnatural movements.

On the other side, if bilater interaction is not required, a vision-based interface is preferable. A vision-based interface does not require physical contact with external devices such as cables, connectors and objects outside of the user working area. This grants a more natural and intuitive interaction, which is reflected on the task performance: as shown in \cite{Kofman2005}, the accuracy of object gripping tasks is improved by mean of a contactless vision-based robot teleoperation method; in \cite{Livatino2009} a stereo vision system improved the performance of a mobile robot teleoperation application and in \cite{CollabWork} a vision and gestures-based communication has been adopted to robustly achieve several task in collaboration with the human operator. 

Furthermore, if a vision-interface is integrated with virtual and augmented reality techniques, it translates in a greater level of immersion for the user. Such techniques are used to improve the feedback information; in fact, the operator feels like being physically present in the remote environment, improving the immersion level. Immersivity is, in fact, one of the most important reasons for using virtual and augmented reality \cite{Boboc2012}.
An augmented reality system for teleoperation based on the Leap Motion (LM) controller is presented in \cite{Peppoloni2015}. For its application domain, the LM controller is considered an accurate sensor \cite{Hedayati2018}, however it is limited to a relatively small measuring distance if compared with other sensors. In this sense, the LM controller introduces spatial constraints that clash with the user immersivity concept.

\begin{figure*}[b!]
	\centering
	\includegraphics[width=1\textwidth]{figures/horizcalib}
	\caption{Scheme of the calibration steps.}
	\label{fig:wscalib}
\end{figure*}

For these reasons, we present a novel robot augmented reality teleoperation system that exploits RGB cameras, which supply greater measuring distance if compared with the LM controller. A ROS-based framework has been developed to supply hand tracking and hand-gesture recognition features, exploiting the OpenPose software \cite{simon2017hand, cao2018openpose} based on the Deep Learning framework Caffe \cite{jia2014caffe}.
This, in combination with the ease of availability of an RGB camera, leads the framework to be strongly open-source oriented and highly replicable on all the ROS-based platform. The proposed system includes: a rigorous procedure for robot workspace calibration and a mapping policy between the coordinate system of the user and the robot (Section \ref{sec:calib}), and a deep learning approach for the hand-gesture recognition by means of hand keypoints estimation (Section \ref{sec:HG_recon}).
Different experiments were performed on a \textit{Sawyer (Rethink Robotics)} industrial collaborative robot to evaluate repeatability and accuracy of the proposed system. Section \ref{sec:experiments} reports the results.

\section{Workspace Calibration and Mapping} \label{sec:calib}

Our set-up is composed of two workspaces: the \textit{user workspace} and the \textit{robot workspace}. Cartesian points in the user workspace that (i) may be reached by the user hand and (ii) that are correctly viewed by the camera, correspond to precise robot end-effector Cartesian points in the robot workspace. To obtain the mapping between the hand positions and the robot end-effector positions, it is necessary to perform a set of calibration procedures described in detail in the following sections.

\subsection{User Workspace Calibration}
In the user workspace an RGB camera is used to recognize the hand skeleton in real-time. Therefore, it is necessary to properly calibrate the camera relative to the user-defined reference system. This procedure is called \textit{camera calibration}, and it may be easily realized following standard procedures, see \cite{CameraCalib}.
The projection mapping for a generic point $\mathbf{P}_{C} = (u,v)$ in the camera image plane, with reference frame $C$ to its corresponding real world coordinate point $\mathbf{P}_{H} = (x,y,z)$ in reference frame $H$, is defined by the following formula:
\begin{equation}
s 
\begin{bmatrix}
u \\
v \\
1
\end{bmatrix}
=
\mathbf{K}
\begin{bmatrix}
\mathbf{R} | \mathbf{t}
\end{bmatrix}
\begin{bmatrix}
x \\
y \\
z \\
1
\end{bmatrix}
\label{eq1}
\end{equation}

where homogeneous coordinates are used. However, since we are looking for the position of point $\mathbf{P}_H$ in frame $H$ by back-projecting a 2D point to 3D, it is necessary to invert equation (\ref{eq1}):
\begin{equation}
\mathbf{P}_{H} = \begin{bmatrix}
x \\
y \\
z
\end{bmatrix}=
\left(s 
\begin{bmatrix}
u \\
v \\
1
\end{bmatrix}
\mathbf{K}^{-1}-\mathbf{t}\right)\mathbf{R}^{-1}
\label{eq2}
\end{equation}

In the equations above, the scalar $s\in\mathbb{R}$ is the scale factor of the image, $\mathbf{K}\in\mathbb{R}^{3\times3}$ is the camera matrix containing the intrinsic parameters of the camera, such as focal length and optical center obtained through the calibration procedure, $\mathbf{[R|t]}\in\mathbb{R}^{3\times4}$ is the rigid transformation matrix containing the exstrinsic parameters for rotation ($\mathbf{R}\in\mathbb{R}^{3\times3}$) and translation ($\mathbf{t}\in\mathbb{R}^3$) of the camera reference frame $C$ relative to the calibration master reference frame $H$. 
To obtain matrix \textbf{K} it is necessary to perform a calibration procedure. A well-performed calibration procedure allows to obtain a satisfactory estimation of the camera parameters. To correctly map image points to the corresponding real world coordinates, the rigid transformation matrix must be estimated with respect to the user-defined reference system of the calibration master. Thus, if reference frame $H$ changes or the camera frame $C$ moves, it is necessary to estimate the correct rigid transformation matrix again. This procedure has been automatized by our software: a calibration script calculates matrix \textbf{K} using a set of calibration images acquired by the user, then it takes a new frame from the actual set-up to estimate the position of reference system $H$.

Images acquired by the camera are processed and the hand skeleton joints coordinates are calculated. By using the abovementioned formulas, it is possible to obtain the real world coordinates of the hand skeleton joints in real time for each acquired frame (image frame in Fig. \ref{fig:wscalib}).


\subsection{Robot Workspace Calibration}

The robot workspace refers to the space in which the robot moves (reference system $W$ of Fig. \ref{fig:wscalib}) with respect to the user workspace (reference system $H$ of Fig. \ref{fig:wscalib}). In this case, the user hand real-world position in reference system $H$ is mapped to the new reference system $W$. The mapping between reference system $H$ and reference system $W$ is obtained easily if the two workspaces have the same dimension (matrix \textbf{[R$|$t]} is the identity matrix) or if one workspace is a scaled version of the other one (matrix \textbf{[R$|$t]} is the identity matrix multiplied by the scale factor). %verificare

To correctly move the robot in a cartesian position of reference system $W$, it is necessary to perform a calibration between reference system $W$ and the robot reference system $R$. This procedure has been carried out experimentally by moving the robot (using its manual guidance mode) in different Cartesian positions of reference system $W$. The robot correct positioning on top of each calibration position has been assured by using a 3D-printed centering tool (Fig. \ref{fig:robotcalib}). 
\begin{figure}[b]
	\centering
	\includegraphics[width=1\columnwidth]{figures/robotcalibration}
	\caption{Example of the robot calibration procedure. The calibration pose tool (top-right) is placed in correspondence of a calibration marker, then the robot end-effector is carefully placed inside the tool (bottom-left). The calibration pose tool cavity has been purposely made to fit the robot end-effector.}
	\label{fig:robotcalib}
\end{figure}

The tool must be centered manually on each calibration marker and secured in place, then the robot end effector can be moved on it and carefully positioned inside the purposely made circular cavity of the tool. When the positioning is complete, the robot coordinates (both in the Cartesian space and in the Joints space) corresponding to that particular marker (of which the positioning is known with respect to reference system $W$) can be extracted using ROS or the robot proprietary software. 

When a satisfactory number of calibration positions has been acquired, it is possible to estimate the rigid transformation matrix between workspaces $W$ and $R$. The proposed system currently involves a planar motion, thus, the mapping procedure does not consider the $z$ axis.

Referring to the calibration example in Fig. \ref{fig:master_robot}, the plane position of a point $\mathbf{P}_1\in\mathbb{R}^2$ is calculated with respect to both frame $W$ ($P_{1,W}$) and frame $R$ ($P_{1,R}$).

\begin{figure}[h]
	\centering
	\includegraphics[width=1\columnwidth]{figures/calibration}
	\caption{Master used to calibrate the second user-defined reference system $W$ with the robot reference frame $R$. The Figure illustrate the position of  point $\mathbf{P_1}$ in both reference frames. To properly calibrate the system, the position of each point is required, both for frame $W$ and $R$. In the procedure, 13 calibration points have been acquired.}
	\label{fig:master_robot}
\end{figure}

The distance between the two reference frames is $\mathbf{T}_R\in\mathbb{R}^2$, hence:

\begin{equation}
\mathbf{P}_{1,R} = \mathbf{P}_{1,W} + \mathbf{T}_R
\end{equation}

Using homogeneous coordinates it is possible to rewrite the previous equation as matrix products:

\begin{equation}\label{eq:matrix_M}
\mathbf{P}_{1,R} = \mathbf{M}^W_{R}\mathbf{P}_{1,W}
\end{equation}

Where $\mathbf{M}^W_{R}\in\mathbb{R}^{3\times3}$ is the rigid transformation matrix between the two reference systems. By outlining equations from (\ref{eq:matrix_M}), we obtain:
\begin{equation}\label{eq:linear_sys1}
\begin{cases}
x_{P_{1,W}}=x_{P_{1,R}}\cos\theta+y_{P_{1,R}}\sin\theta+x_{T_R}\\
y_{P_{1,W}}=-x_{P_{1,R}}\sin\theta+y_{P_{1,R}}\cos\theta+y_{T_R}
\end{cases}
\end{equation}

The aim of the calibration procedure is to calculate $\mathbf{M}^W_{R}$ in order to find the correct position and orientation of reference frame of robot workspace $W$ with respect to the frame $R$. However, considering only one calibration position point $\mathbf{P}_1$, the system in (\ref{eq:linear_sys1}) results underdetermined, hence, a minimum of $n>2$ calibration points is required to solve the system. To minimize the calibration error, $n=13$ points have been considered. Thus, the system in (\ref{eq:linear_sys1}) becomes an overdetermined system $\mathbf{A}\mathbf{x}=\mathbf{b}$ that has been solved using the least square method, such as:

\begin{equation}\label{eq:linear_sys2}
\begin{gathered}
\mathbf{A} = \begin{bmatrix}
x_{P_{0,R}} & y_{P_{0,R}} & 1 & 0\\
-y_{P_{0,R}} & x_{P_{0,R}} & 0 & 1 \\
\vdots & \vdots & \vdots & \vdots\\
x_{P_{n-1,R}} & y_{P_{n-1,R}} & 1 & 0\\
-y_{P_{n-1,R}} & x_{P_{n-1,R}} & 0 & 1 
\end{bmatrix} , \\ 
\mathbf{x} =   \begin{bmatrix}
\cos\theta\\
\sin\theta\\
x_{T_R}\\
y_{T_R}
\end{bmatrix},\thickspace
\mathbf{b} =   \begin{bmatrix}
x_{P_{0,W}}\\
y_{P_{0,W}}\\
\vdots\\
x_{P_{n-1,W}}\\
y_{P_{n-1,W}}\\
\end{bmatrix}
\end{gathered}
\end{equation}

The rigid transformation matrix $\mathbf{M}^W_{R}$ used to identify the reference frame $W$ from $R$ is defined by the components of $\mathbf{x}$.
Considering the overall schema in Fig. \ref{fig:wscalib}, the generic point $\mathbf{P}\in\mathbb{R}$ in the robot reference frame $R$ with respect to the camera frame $C$ is calculated as follows:

\begin{equation}\label{eq:roto1}
\mathbf{P}_{R} = \mathbf{M}^W_{R}\mathbf{P}_{W}
\end{equation}

The same point in the robot workspace $W$ is:

\begin{equation}\label{eq:roto2}
\mathbf{P}_{W} = K_s\mathbf{P}_{H}
\end{equation}

Where $K_s\in \mathbb{R}$ is the a scaling factor between the robot and the user workspaces and $\mathbf{P}_{H}$ is defined in (\ref{eq2}). Finally, considering equations (\ref{eq:roto1}),(\ref{eq:roto2}) and equation (\ref{eq2}), the resulting point $\mathbf{P}_R$ in the robot reference frame using the camera coordinates is calculated as:

\begin{equation}
\mathbf{P}_{R} = \begin{bmatrix}
x \\
y \\
z
\end{bmatrix}=
K_s\mathbf{M}^W_{R}
\left(s 
\begin{bmatrix}
u \\
v \\
1
\end{bmatrix}
\mathbf{K}^{-1}-\mathbf{t}\right)\mathbf{R}^{-1}
\end{equation}

The space coordinates $(u,v)$ are the output of the hand-gesture recognition algorithm, while Cartesian coordinates $(x,y,z)$ are the position set-points for the robot.

\section{Hand-Gesture Recognition}\label{sec:HG_recon}

The proposed teleoperation method is based on the recognition of the user hands skeleton.

Each frame acquired by the RGB camera (in our set-up, a Kinect v2 camera) is processed by the software, which leverages the OpenPose hand skeleton recognition network to predict the hand skeleton, following the details of \cite{simon2017hand}.
The gesture recognition procedure is based on the position of the reference keypoint (red keypoint $0$ in Fig. \ref{fig:keypoints}) and on the position of the four knuckles keypoints (blue keypoints $5, 9, 13, 17$ in Fig. \ref{fig:keypoints}). We defined two gestures used to carry out basic teleoperation tasks: the \textbf{open hand} gesture (Fig. \ref{fig:keypoints}, top-right) and the \textbf{index} gesture (Fig. \ref{fig:keypoints}, bottom-right).

We based the gesture recognition procedure on the recognition of the fingers, which may be opened or closed. To robustly recognize if a finger is opened or closed, the procedure is the following:
\begin{enumerate}
\item check if all the keypoints of the considered finger have been correctly predicted by the network (considering a prediction score threshold of $40\%$);
\item in the case of the considered finger, check the fingertip distance from reference keypoint $0$ ($D_{0,F}$) and the knuckle distance from reference keypoint $0$ ($D_{0,K}$) expressed as a percentage of the fingertip distance from reference keypoint $0$ ($\frac{(D_{0,F} - D_{0,K})}{D_{0,F}}$). If this distance is less than $10\%$, the finger keypoints are collapsed around the knuckle keypoint, thus the finger is considered closed;
\item check the Euclidean distances between the reference keypoint $0$ and the last keypoint of each finger (pink keypoints $8, 12, 16, 20$ in Fig. \ref{fig:keypoints}). If the calculated distance of the index finger is greater than the others, the index finger is considered as opened with priority. This requirement has been proved useful to reduce the recognition error of the index gesture due to a wrong prediction of the fingers keypoints.
\end{enumerate}

\begin{figure} [ht]
  \centering
  \includegraphics[width=1\columnwidth]{figures/keypoints}
  \caption{Scheme of the hand skeleton predicted by OpenPose neural network. The red keypoint is the reference keypoint, the blue keypoints are the knuckles keypoints and the pink keypoints are the fingertips keypoints. Examples of correclty recognized gestures: open hand gesture (top-right) and index gesture (bottom-right).}
  \label{fig:keypoints}
\end{figure}

Considering the calibration procedure detailed in Section \ref{sec:calib}, a certain position $\mathbf{P}_{H}$ of user workspace $H$ corresponds to a certain robot end-effector position $\mathbf{P}_{W}$ in workspace $W$. Hence, to move the robot end-effector in position $P_W$  using the software, the users must: %img
\begin{enumerate}
\item place their hand in position $\mathbf{P}_H$ (corresponding to position $\mathbf{P}_W$), using the real-time visualization of the software as guidance (Fig. xx a);
\item perform the open hand gesture to allow the coordinate extraction (Fig. xx b);
\item perform the index gesture by carefully pointing the index finger to position $\mathbf{P}_H$ (Fig. xx c).
\end{enumerate}

It is worth noting that, since the hand skeleton is obtained by a neural network which estimates the joints coordinates frame per frame, their position in consectuive frames may vary. Therefore, our software extracts $N$ different $\mathbf{P}_H$ coordinates from $N$ consecutive index gestures recognized in consecutive frames. The average coordinates are extracted to reduce positioning errors introduced by the hand skeleton recognition network. The higher the value of $N$, the higher the error reduction, at the cost of a higher delay before the final $\mathbf{P}_H$ coordinates are extracted. In our set-up, we set $N = 7$.
After a position $\mathbf{P}_H$ is obtained, the corresponding robot position $\mathbf{P}_R$ is calculated and the robot is moved there using the ROS interface.
To perform a new robot movement, the procedure in Fig. xx must be repeated from the start. %%

\section{EXPERIMENTAL EVALUATION}\label{sec:experiments}

A reliable teleoperation system is obtained if the robot correctly moves to the desired position with a low positioning error. In the proposed set-up, the positioning error is obtained as a sum of different errors, as shown in Fig. \ref{fig:wscalib}.

First, when the user points the index finger to a Cartesian point in workspace $H$, OpenPose neural network estimates the index position in the image as a point $\mathbf{P}_O \in \mathbb{R}$ (keypoint $8$ in Fig. \ref{fig:keypoints}). According to the filtering adopted and explained in Section \ref{sec:HG_recon}, the correspoding point is:

\begin{equation}
\mathbf{P}_O = \begin{bmatrix}
\frac{1}{N}\sum_{n=1}^{N}x_{P_{On}} \\
\frac{1}{N}\sum_{n=1}^{N}y_{P_{On}}
\end{bmatrix}
\thickspace \forall\thickspace 0 \leq n \leq N
\end{equation}

The extracted index position $\mathbf{P}_O$ [px] corresponds to the camera image point $\mathbf{P}_C$ with an estimation error $\mathbf{E}_{sk}$, obtained as the pixel distance between the real index position ($\mathbf{P}_C$) and the estimaed index keypoint position ($\mathbf{P}_O$):

\begin{equation}
\mathbf{P}_C = \mathbf{P}_O + \mathbf{E}_{sk} = \begin{bmatrix}
\frac{1}{N}\sum_{n=1}^{N}x_{P_{On}} \\
\frac{1}{N}\sum_{n=1}^{N}y_{P_{On}}
\end{bmatrix} + \mathbf{E}_{sk}
\end{equation}

Thanks to the camera calibration procedure, a point in the acquired image frame $\mathbf{P}_C$ [px] corresponds to a Cartesian point $\mathbf{P}_C'$ [m] rototranslated in workspace $H$. $\mathbf{P}_C'$ corresponds to the real position $\mathbf{P}_H$ of the original $\mathbf{P}_C$ with an estimation error $\mathbf{E}_H$ which depends on the accuracy of the calibration. In the following, we refer to generic points $\mathbf{P}'$ as the already rototranslated points according to equations (\ref{eq2}), (\ref{eq:roto1}, (\ref{eq:roto2}). Thus, $\mathbf{P}_H$ is defined as:

\begin{equation}
\mathbf{P}_H = \mathbf{P}_C' + \mathbf{E}_H = (\mathbf{P}_O + \mathbf{E}_{sk})' + \mathbf{E}_H
\end{equation}

Since the image point we consider is $\mathbf{P}_O$, we obtain: 

\begin{equation}
\mathbf{P}_H = \left(\begin{bmatrix}
\frac{1}{N}\sum_{n=1}^{N}x_{P_{On}} \\
\frac{1}{N}\sum_{n=1}^{N}y_{P_{On}}
\end{bmatrix} + \mathbf{E}_{sk} \right)' + \mathbf{E}_H
\end{equation}

where the apex represents the conversion from pixels to meters (Section \ref{sec:calib}).

The workspace where we want to move the robot is workspace $W$, therefore we must obtain the position of $\mathbf{P}_W$, which corresponds to $\mathbf{P}_H$ according to the specific mapping between the two workspaces. We obtain:

\begin{equation}
\mathbf{P}_W = \mathbf{P}_H' + \mathbf{E}_{map}
\end{equation}
\begin{equation}
\mathbf{P}_W = \left(\begin{bmatrix}
\frac{1}{N}\sum_{n=1}^{N}x_{P_{On}} \\
\frac{1}{N}\sum_{n=1}^{N}y_{P_{On}}
\end{bmatrix} + \mathbf{E}_{sk} + \mathbf{E}_{H}\right)' + \mathbf{E}_{map}
\end{equation}

where $\mathbf{E}_{map}$ is the error caused by the mapping.

Finally, to correctly move the robot end-effector to $\mathbf{P}_W$, we must obtain the corresponding $\mathbf{P}_R$ in the robot reference system $R$. This correspondence is obtained from the robot calibration procedure detailed in Section \ref{sec:calib}: 

\begin{equation}
\mathbf{P}_R = \mathbf{P}_W' + \mathbf{E}_W
\end{equation}
\begin{equation}\label{eq:error_final}
\mathbf{P}_R = \left(\begin{bmatrix}
\frac{1}{N}\sum_{n=1}^{N}x_{P_{On}} \\
\frac{1}{N}\sum_{n=1}^{N}y_{P_{On}}
\end{bmatrix} + \mathbf{E}_{sk} + \mathbf{E}_{H} + \mathbf{E}_{map} \right)' + \mathbf{E}_W
\end{equation}

It is made evident that moving from one reference frame to another introduces errors. Considering (\ref{eq:error_final}), and $\mathbf{E}_{map}$ assumed equal to zero (because we kept workspace $H$ and workspace $W$ dimensions equal), we designed two experiments to assess if the positioning error obtained depends (i) on the estimation of the hand skeleton ($\mathbf{E}_{sk}$), (ii) on the camera calibration ($\mathbf{E}_{H}$) or (iii) on the robot workspace calibration and robot intrinsic characteristics ($\mathbf{E}_{W}$).

\subsection{Evaluation of the camera calibration error}
Here there is the camera calibration error. The following table is just a sample

\begin{table}[h!]
	\centering
	\caption{Norm, averages and standard deviations of error $\mathbf{E}_H$}
	\label{tab:laser}
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{cccccc}
			\hline
			\multirow{2}{*}{Points} & \multicolumn{2}{c}{$\mathbf{E}_H$} &  & \multicolumn{2}{c}{$\overline{\mathbf{E}}_H$} \\ \cline{2-3} \cline{5-6} 
			& $x$                & $y$               &  & $x$                     & $y$                     \\ \hline
			A4                      & 0.40               & 0.50              &  & 0.20                    & 1.50                    \\
			A5                      & 2.00               & 0.50              &  & 0.91                    & 1.40                    \\
			O1                      & 6.70               & 0.70              &  & 1.63                    & 0.58                    \\
			O0                      & 0.60               & 0.40              &  & 0.77                    & 1.44                    \\
			O2                      & 2.60               & 0.20              &  & 0.70                    & 1.15                    \\
			B4                      & 0.30               & 1.10              &  & 0.26                    & 1.86                    \\
			B5                      & 1.70               & 0.00              &  & 0.19                    & 1.59                    \\ \hline
		\end{tabular}%
	\end{adjustbox}
\end{table}

\subsection{Evaluation of the skeleton estimation error}

The positioning error due to the estimation of the hand skeleton $\mathbf{E}_{sk}$ made by OpenPose neural network has been evaluated considering the theoretical position of the index in the image $\mathbf{T} \in \mathbb{R}^2$ and the index joint position in the image $\mathbf{A} \in \mathbb{R}^2$ calculated by the software (Fig. xx) such as:

\begin{equation}
\mathbf{E}_{sk} = \mathbf{T} - \mathbf{A} = \begin{bmatrix}
T_{x,n}\\
T_{y,n}
\end{bmatrix}-
\begin{bmatrix}
A_{x,n}\\
A_{y,n}
\end{bmatrix}
\thickspace
\forall \thickspace 1 \leq n \leq N
\end{equation}

When users point to a position, they must keep the index gesture firmly in place until $N = 7$ consecutive index gesture have been successfully detected by the software. Hence, in this experiment the user hand moved to $14$ different locations of workspace $H$, corresponding to $14 * 7 = 98$ couples of image frames and index joint estimations.

Theoretical positions have been manually selected from each acquired frame considering the tip of the index finger, while the actual positioning $A$ of each frame corresponds to the predicted index keypoint obtained from OpenPose neural network. The user hand in the acquired frames is both vertically oriented and left or right oriented. An equal number of left-hand and right-hand frames have been selected for the evaluation.

To evaluate the relevance of $\mathbf{E}_{sk}$ we considered the average value $\overline{\mathbf{E}}_{sk}$ and the standard deviation $\sigma{\mathbf{E}}_{sk}$ along all 7 software detection for all 14 points (Table \ref{tab:dev}).
\begin{table}[h!]
	\centering
	\caption{Averages and standard deviations of error $\mathbf{E}_{sk}$, theoretical position $\mathbf{T}$ and actual position $\mathbf{A}$}
	\label{tab:dev}
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{@{}ccccclccccc@{}}
			\toprule
			\multicolumn{2}{c}{$\overline{\mathbf{E}}_{sk}$[px]} &  & \multicolumn{2}{c}{$\sigma{\mathbf{E}}_{sk}$[px]} &  & \multicolumn{2}{c}{$\sigma\mathbf{T}${[px]}} &  & \multicolumn{2}{c}{$\sigma\mathbf{A}${[px]}} \\ \cmidrule(r){1-2} \cmidrule(lr){4-5} \cmidrule(lr){7-8} \cmidrule(l){10-11} 
			$x$                     & $y$                    &  & $x$                   & $y$                   &  & $x$                    & $y$                   &  & $x$                    & $y$                   \\ \midrule
			-1.29                   & -3.29                  &  & 3.92                  & 4.06                  &  & 47.8                   & 6.80                  &  & 48.9                   & 9.05                  \\
			1.71                    & -2.14                  &  & 3.06                  & 3.00                  &  & 82.3                   & 67.8                  &  & 80.4                   & 68.0                  \\
			-0.29                   & -4.86                  &  & 4.49                  & 6.49                  &  & 41.6                   & 22.5                  &  & 40.2                   & 28.2                  \\
			1.43                    & -4.14                  &  & 2.77                  & 5.19                  &  & 52.5                   & 6.25                  &  & 53.8                   & 8.34                  \\
			2.29                    & 0.14                   &  & 4.43                  & 1.96                  &  & 4.43                   & 3.59                  &  & 0.00                   & 4.55                  \\
			0.43                    & -5.43                  &  & 2.56                  & 1.18                  &  & 2.56                   & 1.18                  &  & 0.00                   & 0.00                  \\
			4.14                    & -6.14                  &  & 4.09                  & 2.53                  &  & 78.1                   & 87.0                  &  & 78.6                   & 86.2                  \\
			0.29                    & 0.00                   &  & 3.73                  & 4.07                  &  & 151                    & 26.5                  &  & 147                    & 30.2                  \\
			3.71                    & -3.00                  &  & 3.84                  & 3.82                  &  & 97.8                   & 15.1                  &  & 98.5                   & 15.1                  \\
			1.29                    & -0.43                  &  & 3.92                  & 1.84                  &  & 77.9                   & 21.9                  &  & 76.4                   & 23.5                  \\
			4.14                    & -1.43                  &  & 4.09                  & 4.03                  &  & 79.0                   & 20.7                  &  & 78.3                   & 23.8                  \\
			3.29                    & -1.71                  &  & 4.46                  & 2.55                  &  & 8.91                   & 43.8                  &  & 11.8                   & 44.5                  \\
			4.71                    & -3.00                  &  & 2.37                  & 4.24                  &  & 2.26                   & 14.4                  &  & 4.55                   & 14.0                  \\
			3.00                    & -1.57                  &  & 2.93                  & 3.96                  &  & 27.0                   & 98.7                  &  & 28.5                   & 97.2                  \\ \bottomrule
		\end{tabular}%
	\end{adjustbox}
\end{table}

In each location, the $(x,y)$ components of the average error has been calculated as:

\begin{equation}
\overline{\mathbf{E}}_{sk} = \begin{bmatrix}
\overline{E}_{sk,x}  = \frac{1}{N}\sum_{n=1}^{N} (T_{xn} - A_{xn}) \\
\overline{E}_{sk,y} = \frac{1}{N}\sum_{n=1}^{N} (T_{yn} - A_{yn})
\end{bmatrix}
\end{equation}

It is worth noting that the mean value calculated with all $\overline{\mathbf{E}}_{sk}$ is extremely low: $2.06$ [px] along the X-axis and $-2.64$ [px] along the Y-axis, as well for the mean of standard deviations $\sigma{\mathbf{E}}_{sk}$: $3.61$ [px] along the X-axis and $3.49$ [px] along the Y-axis. The negative sign represents the case when the actual positioning $A$ is overstimated with respect to the corresponding theoretical positioning $T$.

Moreover, by calculating the standard deviation $\sigma_x$ of the $N$ different pixel positions along both for theoretical positions ($\sigma_{Tx}$, $\sigma_{Ty}$) and actual positions ($\sigma_{Ax}$, $\sigma_{Ay}$) (Table \ref{tab:dev}), we know how much the user kept the hand firmly in place for each location. This result is an index useful to understand in which location the user moved the hand too much, thus reducing the accuracy of the estimation of the average index keypoint, which could lead to an incorrect placing of the robot end-effector.

%
%\begin{table}[h]
%\begin{center}
%\caption{Test Locations Standard Deviations}
%\label{tab:firm}
%\begin{tabular}{|c||c||c||c|}
%\hline
%$\sigma_{Tx}$[px] & $\sigma_{Ty}$[px] & $\sigma_{Ax}$[px] & $\sigma_{Ay}$[px] \\
%\hline
%47.83 & 6.80 & 48.85 & 9.05\\
%82.25 & 67.77 & 80.35 & 68.02\\
%41.62 & 22.46 & 40.18 & 28.16\\
%52.52 & 6.25 & 53.76 & 8.34\\
%4.43 & 3.59 & 0.00 & 4.55\\
%2.56 & 1.18 & 0.00 & 0.00\\
%78.10 & 87.03 & 78.57 & 86.21\\
%150.60 & 26.54 & 146.98 & 30.17\\
%97.83 & 15.08 & 98.48 & 15.09\\
%77.88 & 21.88 & 76.35 & 23.49\\
%79.02 & 20.71 & 78.26 & 23.78\\
%8.91 & 43.75 & 11.75 & 44.54\\
%2.26 & 14.37 & 4.55 & 14.04\\
%27.01 & 98.68 & 28.53 & 97.18\\
%\hline
%\end{tabular}
%\end{center}
%\end{table}
\subsection{Evaluation of the robot workspace error}

In our set-up, reference system $H$ is placed horizontally with the camera mounted at a $1$ m distance (green horizontal square in Fig. \ref{fig:wscalib}). Reference system $W$, however, has been placed vertically on a glass pane (purple vertical square in Fig. \ref{fig:wscalib}).

The robot workspace error $\mathbf{E}_{W}$ is composed by an error $\mathbf{E}^a_{W}$ only related to the calibration procedure, and the robot position error $\mathbf{E}^b_{W}$ due to physical robot characteristics, such that:
\begin{equation}
\mathbf{E}_{W} = \mathbf{E}^a_{W} + \mathbf{E}^b_{W}
\end{equation} 
To robustly assess the robot positioning error $\mathbf{E}^b_{W}$ of the end-effector, a 3D printed carrier holding a bright red laser has been mounted on the end effector (\textit{Lasiris laser 635nm, 10mW}) (Fig. \ref{fig:laser}, bottom-right corner).

When the robot is moved with its own software to a certain theoretical position $T$, the laser will point to its actual positioning $A$. To correctly visualize and measure the robot workspace and the laser positioning, an RGB camera (\textit{IDS Imaging UI-1460C}) has been mounted behind the glass pane.
A measuring software has been developed using LabVIEW to measure the distance ($E$) between the theoretical position $T$ (calculated as the centroid of the black square of the experimental master, represented as the green dot in Fig. \ref{fig:laser}, top-left corner) and the actual positioning $A$ (red dot in Fig. \ref{fig:laser}, top-left corner).

\begin{figure} [ht]
  \centering
  \includegraphics[width=0.7\columnwidth]{figures/lasermount}
  \caption{Example of a robot positioning evaluation. The image shows a close-up of the 3D printed laser carrier used (bottom-right) and the corresponding view of the measuring software developed (top-left).}
  \label{fig:laser}
\end{figure}

We moved the robot in $7$ theoretical positions, corresponding to markers P3, P4, P5, P6, P7, P8 and P9 in Fig. \ref{fig:master_robot}, for a total of $3$ times per each theoretical position. To do that, we used the Cartesian positions corresponding to the circular markers centroids (theoretical positions) to move the robot using the ROS interface. Hence, this procedure avoids considering the hand skeleton estimation errors.

The error $\mathbf{E}^a_{W}$ has been calculated by considering the same positions previously stated in respect with the robot frame, transformed with Equation \ref{eq:roto1}. Since the rigid transformation matrix is obtained from the calibration least square method, this procedure permits to isolate the calibration error. $\mathbf{E}^a_{W}$ is defined as the absolute difference between the robot theoretical position $T$ and the actual position $A$ calculated using Equation \ref{eq:roto1}.
The resulting distances in millimeters are reported in Table \ref{tab:laser}, where $\mathbf{E}^b_{W}$ values are reported as the average among the 3 tests.


\begin{table}[h!]
	\centering
	\caption{Norm, averages and standard deviations of error $\mathbf{E}^b_{W}$}
	\label{tab:laser}
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{ccccccccc}
			\hline
			\multirow{2}{*}{Points} & \multicolumn{2}{c}{$\mathbf{E}^a_{W}$ [mm]} &  & \multicolumn{2}{c}{$\mathbf{E}^b_{W}$ [mm]} &  & \multicolumn{2}{c}{$\sigma\mathbf{E}^b_{W}$ [mm]} \\ \cline{2-3} \cline{5-6} \cline{8-9} 
			& $x$                & $y$               &  & $x$                     & $y$                     &  & $x$                   & $y$                  \\ \hline
			P8                     & 0.40               & 0.50              &  & 0.20                    & 1.50                    &  & 0.17                  & 0.06                 \\
			P9                      & 2.00               & 0.50              &  & 0.91                    & 1.40                    &  & 0.40                  & 0.04                 \\
			P5                      & 6.70               & 0.70              &  & 1.63                    & 0.58                    &  & 0.14                  & 0.05                 \\
			P6                      & 0.60               & 0.40              &  & 0.77                    & 1.44                    &  & 0.04                  & 0.05                 \\
			P7                      & 2.60               & 0.20              &  & 0.70                    & 1.15                    &  & 0.02                  & 0.12                 \\
			P3                      & 0.30               & 1.10              &  & 0.26                    & 1.86                    &  & 0.01                  & 0.05                 \\
			P4                      & 1.70               & 0.00              &  & 0.19                    & 1.59                    &  & 0.11                  & 0.27                 \\ \hline
		\end{tabular}%
	\end{adjustbox}
\end{table}

We computed the resulting mean distance and the standard deviation of each marker position, reported in Table \ref{tab:laser}. Considering $\mathbf{E}^a_{W}$, the average mean distance is $2.04$ [mm] for the X-axis and $0.49$ [mm] for the Y-axis. While $\mathbf{E}^b_{W}$ has an average of $0.66$ [mm] for the X-axis and $1.66$ [mm] for the Y-axix. The standard deviation among the 3 tests for $\mathbf{E}^b_{W}$ is $0.13$ [mm] for the X-axis and $0.09$ [mm] for the Y-axis.

\section{CONCLUSIONS}
%%%%%%% TO DO

Conclusioni sul progetto/esperimenti ottenuti.
Problematiche incontrate e come sono state risolte.
Future developments.

\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.
%Hands-free
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{ieeetr}
\bibliography{bibliography}


\end{document}